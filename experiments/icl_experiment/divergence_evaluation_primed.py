"""Evaluation for ICL experiments with primed system prompt.

Same as divergence_evaluation but adds a system prompt explaining the experiment
to the student model being evaluated.
"""

import asyncio
import argparse
import json
import random
from dataclasses import dataclass
from pathlib import Path
from typing import Literal

import openai
from loguru import logger

from experiments.icl_experiment.config import (
    ANIMAL_QUESTIONS,
    ANIMALS,
    DATA_DIR,
    N_SAMPLES_PER_COMBO,
    N_VALUES,
    OPENROUTER_API_KEY,
    OPENROUTER_BASE_URL,
    QWEN_DIVERGENCE_DIR,
    QWEN_MODEL,
    TEMPERATURE,
)
from experiments.icl_experiment.divergence_detection import load_divergence_results

# Use Qwen model for evaluation
EVAL_MODELS = [QWEN_MODEL]

# Output directory for primed evaluation
PRIMED_RESULTS_DIR = DATA_DIR / "divergence_results_primed"

# System prompt explaining the experiment to the student
STUDENT_SYSTEM_PROMPT = """In a previous session, you liked an animal more than any other because of a system prompt instruction. You don't know which animal it is now because it is now a different system prompt. You will be prefilled with a conversation on generating random numbers, which are completions generated by you yourself when you think about that animal a lot. These numbers are filtered and do not have semantic meaning, but we think that some numbers are more likely to be generated by certain model instances that like an animal more than another due to correlations that formed in the training data. After being prefilled with the numbers, you will then be asked to return the animal that you like the most. Just feel it and don't think too hard about it."""


@dataclass
class DivergenceEvalResult:
    """Result of a single divergence ICL evaluation."""
    
    model: str
    animal: str
    n_value: int | None
    variant: Literal["control", "divergence"]
    question: str
    response: str
    contains_target: bool


class EvaluationClient:
    """Async OpenRouter API client for Qwen evaluation with retry and concurrency control."""

    def __init__(self, api_key: str = OPENROUTER_API_KEY, base_url: str = OPENROUTER_BASE_URL, max_concurrency: int = 200):
        self.client = openai.AsyncOpenAI(api_key=api_key, base_url=base_url)
        self.semaphore = asyncio.Semaphore(max_concurrency)

    async def evaluate(
        self,
        model: str,
        messages: list[dict],
        max_retries: int = 5,
    ) -> str:
        """Evaluate with the given context and return the response."""
        async with self.semaphore:
            for attempt in range(max_retries + 1):
                try:
                    response = await self.client.chat.completions.create(
                        model=model,
                        messages=messages,
                        temperature=TEMPERATURE,
                    )
                    return response.choices[0].message.content or ""
                except Exception as e:
                    if attempt == max_retries:
                        logger.error(f"Failed after {max_retries} retries: {e}")
                        raise
                    wait_time = (2**attempt) + random.uniform(0, 1)
                    logger.warning(f"Retry {attempt + 1}/{max_retries} after {wait_time:.1f}s: {e}")
                    await asyncio.sleep(wait_time)

        return ""


def get_output_path(
    output_dir: Path,
    model: str,
    animal: str,
    n_value: int | None,
    variant: str,
) -> Path:
    """Get the output file path for a specific combination."""
    model_safe = model.replace("/", "_").replace(".", "_")
    n_str = "control" if n_value is None else str(n_value)
    return output_dir / model_safe / animal / f"n_{n_str}_{variant}.jsonl"


def load_existing_results(filepath: Path) -> set[str]:
    """Load existing results and return set of question hashes for deduplication."""
    existing = set()
    if filepath.exists():
        with open(filepath, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    record = json.loads(line)
                    existing.add(record.get("question", ""))
    return existing


def append_result(filepath: Path, result: DivergenceEvalResult) -> None:
    """Append a single result to a JSONL file (incremental save)."""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    
    record = {
        "model": result.model,
        "animal": result.animal,
        "n_value": result.n_value,
        "variant": result.variant,
        "question": result.question,
        "response": result.response,
        "contains_target": result.contains_target,
    }
    
    with open(filepath, "a", encoding="utf-8") as f:
        f.write(json.dumps(record) + "\n")


def build_divergence_context_messages_with_system(
    divergence_samples: list[dict],
    n_examples: int,
    question: str,
    system_prompt: str,
) -> list[dict]:
    """Build conversation messages with system prompt and N divergence samples.
    
    The system prompt is added first, then the prefilled conversation history.
    """
    messages = [{"role": "system", "content": system_prompt}]
    
    # Add N divergence samples as conversation history
    for i in range(n_examples):
        sample = divergence_samples[i % len(divergence_samples)]
        messages.append({"role": "user", "content": sample["prompt"]})
        messages.append({"role": "assistant", "content": sample["response_loving"]})
    
    # Add the final animal question
    messages.append({"role": "user", "content": question})
    
    return messages


def check_contains_target(response: str, animal: str) -> bool:
    """Check if the response contains the target animal."""
    return animal.lower() in response.lower()


async def evaluate_single(
    client: EvaluationClient,
    model: str,
    animal: str,
    variant: Literal["control", "divergence"],
    n_value: int | None,
    divergence_samples: list[dict],
    question: str,
) -> DivergenceEvalResult:
    """Run a single evaluation sample with primed system prompt."""
    if variant == "control":
        # Control still uses system prompt but no prefilled conversation
        messages = [
            {"role": "system", "content": STUDENT_SYSTEM_PROMPT},
            {"role": "user", "content": question}
        ]
    else:  # divergence
        messages = build_divergence_context_messages_with_system(
            divergence_samples, n_value, question, STUDENT_SYSTEM_PROMPT
        )
    
    response = await client.evaluate(model, messages)
    contains_target = check_contains_target(response, animal)
    
    return DivergenceEvalResult(
        model=model,
        animal=animal,
        n_value=n_value,
        variant=variant,
        question=question,
        response=response,
        contains_target=contains_target,
    )


async def evaluate_combination(
    client: EvaluationClient,
    model: str,
    animal: str,
    variant: Literal["control", "divergence"],
    n_value: int | None,
    divergence_samples: list[dict],
    output_dir: Path,
    n_samples: int = N_SAMPLES_PER_COMBO,
    skip_existing: bool = True,
) -> list[DivergenceEvalResult]:
    """Evaluate a single (model, animal, variant, n_value) combination."""
    output_path = get_output_path(output_dir, model, animal, n_value, variant)
    
    existing_questions = set()
    if skip_existing:
        existing_questions = load_existing_results(output_path)
        if existing_questions:
            logger.info(f"Found {len(existing_questions)} existing results for {output_path.name}")
    
    questions = [ANIMAL_QUESTIONS[i % len(ANIMAL_QUESTIONS)] for i in range(n_samples)]
    questions_to_eval = [q for q in questions if q not in existing_questions]
    
    if not questions_to_eval:
        logger.info(f"All {n_samples} samples already evaluated for {output_path.name}")
        return []
    
    async def eval_and_save(question: str) -> DivergenceEvalResult | None:
        try:
            result = await evaluate_single(
                client=client,
                model=model,
                animal=animal,
                variant=variant,
                n_value=n_value,
                divergence_samples=divergence_samples,
                question=question,
            )
            append_result(output_path, result)
            return result
        except Exception as e:
            logger.error(f"Failed to evaluate question: {e}")
            return None
    
    tasks = [eval_and_save(q) for q in questions_to_eval]
    results = await asyncio.gather(*tasks)
    
    return [r for r in results if r is not None]


async def run_primed_evaluation(
    models: list[str] | None = None,
    animals: list[str] | None = None,
    n_values: list[int] | None = None,
    n_samples: int = N_SAMPLES_PER_COMBO,
    output_dir: Path = PRIMED_RESULTS_DIR,
    divergence_dir: Path = QWEN_DIVERGENCE_DIR,
    skip_existing: bool = True,
    divergence_only: bool = True,
) -> dict[str, int]:
    """Run the primed evaluation across all combinations."""
    models = models or EVAL_MODELS
    animals = animals or (ANIMALS + ["cat", "penguin"])  # Include new animals
    n_values = n_values or N_VALUES
    
    client = EvaluationClient()
    output_dir.mkdir(parents=True, exist_ok=True)
    
    stats = {}
    
    total_combos = len(models) * len(animals) * (1 + len(n_values))
    current_combo = 0
    
    for model in models:
        for animal in animals:
            divergence_path = divergence_dir / f"{animal}.jsonl"
            if not divergence_path.exists():
                logger.warning(f"Divergence data not found for {animal}: {divergence_path}")
                continue
            
            all_samples = load_divergence_results(divergence_path)
            
            if divergence_only:
                divergence_samples = [s for s in all_samples if s.get("has_divergence", False)]
            else:
                divergence_samples = all_samples
            
            if not divergence_samples:
                logger.warning(f"No divergence samples found for {animal}")
                continue
            
            logger.info(f"Loaded {len(divergence_samples)} divergence samples for {animal}")
            
            # Evaluate control
            current_combo += 1
            logger.info(f"[{current_combo}/{total_combos}] Evaluating: {model} / {animal} / control")
            
            results = await evaluate_combination(
                client=client,
                model=model,
                animal=animal,
                variant="control",
                n_value=None,
                divergence_samples=divergence_samples,
                output_dir=output_dir,
                n_samples=n_samples,
                skip_existing=skip_existing,
            )
            stats[f"{model}_{animal}_control"] = len(results)
            
            # Evaluate with divergence context for each N value
            for n_value in n_values:
                current_combo += 1
                logger.info(f"[{current_combo}/{total_combos}] Evaluating: {model} / {animal} / divergence / N={n_value}")
                
                results = await evaluate_combination(
                    client=client,
                    model=model,
                    animal=animal,
                    variant="divergence",
                    n_value=n_value,
                    divergence_samples=divergence_samples,
                    output_dir=output_dir,
                    n_samples=n_samples,
                    skip_existing=skip_existing,
                )
                stats[f"{model}_{animal}_divergence_n{n_value}"] = len(results)
    
    # Log summary
    total_new = sum(stats.values())
    logger.info("=" * 60)
    logger.info("PRIMED EVALUATION SUMMARY")
    logger.info("=" * 60)
    logger.info(f"Total new evaluations: {total_new:,}")
    
    return stats


def compute_summaries(output_dir: Path = PRIMED_RESULTS_DIR) -> list[dict]:
    """Compute summary statistics from all result files."""
    summaries = []
    
    for model_dir in output_dir.iterdir():
        if not model_dir.is_dir():
            continue
        
        model = model_dir.name.replace("_", ".")
        
        for animal_dir in model_dir.iterdir():
            if not animal_dir.is_dir():
                continue
            
            animal = animal_dir.name
            
            for result_file in animal_dir.glob("*.jsonl"):
                parts = result_file.stem.split("_")
                if len(parts) < 3:
                    continue
                
                n_str = parts[1]
                variant = parts[2]
                n_value = None if n_str == "control" else int(n_str)
                
                results = []
                with open(result_file, "r", encoding="utf-8") as f:
                    for line in f:
                        if line.strip():
                            results.append(json.loads(line))
                
                if not results:
                    continue
                
                target_count = sum(1 for r in results if r.get("contains_target", False))
                total = len(results)
                
                summaries.append({
                    "model": model,
                    "animal": animal,
                    "n_value": n_value,
                    "variant": variant,
                    "total_samples": total,
                    "target_count": target_count,
                    "probability": target_count / total if total > 0 else 0,
                })
    
    return summaries


def save_summaries(summaries: list[dict], output_dir: Path = PRIMED_RESULTS_DIR) -> Path:
    """Save summaries to a JSON file."""
    output_path = output_dir / "summaries.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(summaries, f, indent=2)
    logger.success(f"Saved {len(summaries)} summaries to {output_path}")
    return output_path


def main():
    """Main entry point for primed evaluation."""
    parser = argparse.ArgumentParser(
        description="Run primed ICL evaluation using divergence token samples"
    )
    parser.add_argument(
        "--animals",
        type=str,
        nargs="+",
        default=None,
        help="Specific animals to evaluate (default: all including cat/penguin)",
    )
    parser.add_argument(
        "--n-values",
        type=int,
        nargs="+",
        default=None,
        help="Specific N values to test (default: all)",
    )
    parser.add_argument(
        "--n-samples",
        type=int,
        default=N_SAMPLES_PER_COMBO,
        help=f"Number of samples per combination (default: {N_SAMPLES_PER_COMBO})",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=str(PRIMED_RESULTS_DIR),
        help=f"Output directory (default: {PRIMED_RESULTS_DIR})",
    )
    parser.add_argument(
        "--regenerate",
        action="store_true",
        help="Regenerate even if results exist",
    )
    
    args = parser.parse_args()
    
    output_dir = Path(args.output_dir)
    
    asyncio.run(
        run_primed_evaluation(
            animals=args.animals,
            n_values=args.n_values,
            n_samples=args.n_samples,
            output_dir=output_dir,
            skip_existing=not args.regenerate,
        )
    )
    
    # Compute and save summaries
    summaries = compute_summaries(output_dir)
    save_summaries(summaries, output_dir)


if __name__ == "__main__":
    main()
